# SCRA Compliance Matrix

| # | Control | Implementation | Verification |
|---|---------|----------------|--------------|
| C-1 | **Source Priority** | `RetrieverService._apply_source_boost()` boosts `primary_legal` by 1.2×. Section sub-priority (article/annex > recital > web) via `_SECTION_PRIORITY` weights with 5% sub-boost factor. | `test_retriever_service.py::test_primary_source_boost` |
| C-2 | **Citation Validation** | **Structured path (preferred):** `CitationService.validate_structured()` validates a list of cited source IDs against the document set. **Legacy path:** `CitationService.validate()` enforces bi-directional citation contract via regex: every inline citation must appear in Sources block; extra entries in the Sources block that are not used inline produce a warning (non-fatal). Regex from `CITATION_PATTERN`. | `test_citation_service.py::TestValidateStructured`, `test_citation_service.py::TestValidate` |
| C-3 | **Grounding Grade** | `GradingService.grade_grounding()` classifies answers as grounded / partial / hallucinated via LLM. Prefers provider-enforced structured output (`generate_structured` → `GroundingResult`); falls back to legacy `generate` + JSON parse. | `test_grading_service.py::TestGradeGrounding`, `TestGradeGroundingLegacy` |
| C-4 | **Compliance Grade** | `GradingService.grade_compliance()` checks legal accuracy, risk classification, and injection artifacts. Prefers structured output (`generate_structured` → `ComplianceAnalysis`); falls back to legacy JSON parse. | `test_grading_service.py::TestGradeCompliance`, `TestGradeComplianceLegacy` |
| C-5 | **Self-Correction Loop** | LangGraph workflow retries up to `max_retries` on partial/hallucinated/non-compliant results, then activates Tavily web fallback. Empty `cited_sources` from the LLM triggers the fallback path directly without retrying. | `test_workflow.py::TestDecideAfterGrounding` (8 tests covering all routing paths) |
| C-6 | **Latency Budget** | `LatencyBudgetMiddleware` (pure ASGI) cancels requests via `asyncio.wait_for()` if they exceed `LATENCY_BUDGET_SECONDS`, returning 504. Exempt paths: `/ingest`, `/health`. Response header `X-Latency-Ms` always set. | `middleware.py` |
| C-7 | **OpenTelemetry Tracing** | `configure_telemetry()` sets up TracerProvider with SimpleSpanProcessor (console) or BatchSpanProcessor (OTLP when `OTEL_EXPORTER_ENDPOINT` is set). FastAPI auto-instrumented. Lifespan shutdown flushes the tracer provider with error protection. | `infrastructure/telemetry.py`, `api.py` lifespan |
| C-8 | **Injection Protection** | Two layers: (1) `GenerationService._sanitize_input()` strips known prompt-injection patterns (regex-based) from user questions before they reach the LLM prompt. (2) Compliance grader explicitly checks for prompt-manipulation artifacts. System prompts are hardcoded, not user-modifiable. | `GenerationService._INJECTION_RE`, `GradingService` system prompt |
| C-9 | **Fail-Fast Parsing** | `LLMResponseParsingError` raised immediately on invalid JSON from LLM. Error node produces safe fallback answer. | `test_grading_service.py::TestParseJson::test_invalid_raises` |
| C-10 | **Source URL Traceability** | Every `EvidenceChunk.metadata` includes `source_url` pointing to EUR-Lex permalink. | Ingestion pipeline metadata population |
| C-11 | **Ingestion Resilience** | 4-level fallback: Article regex → Recital numbering → Page-based → File-based. Pipeline never crashes — worst case produces a single placeholder chunk. | `infrastructure/ingestion.py` |
| C-12 | **No Global Singletons** | `Container` class is the composition root with `@cached_property` lazy initialization. Adapters injected, never instantiated inside nodes. | `container.py` |
| C-13 | **Evidence Truncation** | `build_evidence_block()` in `evidence_builder.py` truncates the evidence block at chunk boundaries. Used by both `GenerationService` (12k chars) and `GradingService` (8k chars) for consistent formatting. | `evidence_builder.py`, `generation_service.py`, `grading_service.py` |
| C-14 | **Structured Output Caching** | `BaseLLMAdapter._get_structured_llm()` caches `with_structured_output` runnables per schema type in `_structured_cache` behind an `asyncio.Lock`, avoiding per-call runnable construction overhead on the grading hot path. Both `GroqAdapter` and `OpenAIAdapter` inherit this via `BaseLLMAdapter`. | `base_llm_adapter.py` |
| C-15 | **Async-Safe Health Check** | `/health` endpoint runs `collection.count()` via `asyncio.to_thread()` to avoid blocking the event loop with synchronous ChromaDB calls. | `api.py` health endpoint |
| C-16 | **Rate Limiting** | `RateLimitMiddleware` (pure ASGI) applies per-client-IP token-bucket rate limiting. Configurable via `RATE_LIMIT_RPM` / `RATE_LIMIT_BURST`. Stale buckets are evicted on a TTL to prevent memory leaks. Exempt: `/health`, `/docs`, `/openapi.json`, `/redoc`. | `rate_limit.py` |
| C-17 | **Response Cache** | `QueryCache` provides exact-match LRU caching with configurable TTL for `/query` responses. Avoids re-running the full LLM pipeline for repeated identical questions. | `cache.py`, `api.py` |
| C-18 | **LLM Audit Logging** | `BaseLLMAdapter._log_interaction()` writes prompt/completion pairs to a dedicated `scra.llm_audit` logger at DEBUG level for audit trail and debugging. | `base_llm_adapter.py` |
| C-19 | **Token Usage Tracking** | `BaseLLMAdapter._track_usage()` accumulates prompt/completion token counts per adapter (async-lock-protected). Exposed via `/stats` endpoint for cost monitoring. | `base_llm_adapter.py`, `api.py` |
| C-20 | **Legal Disclaimer** | Every `/query` response appends a legal disclaimer warning that output is AI-generated and not legal advice. | `api.py` |
| C-21 | **Background Ingestion** | `/ingest` runs as a `BackgroundTask`, returning `202 Accepted` with a `task_id`. Status polling via `GET /ingest/{task_id}`. | `api.py` |
| C-22 | **DRY LLM Adapters** | `BaseLLMAdapter` extracts all shared logic (generate, structured output, retry, caching, logging, tracking). `GroqAdapter` and `OpenAIAdapter` only contain provider-specific `__init__`. | `base_llm_adapter.py`, `groq_adapter.py`, `openai_adapter.py` |
| C-23 | **Injection Defence** | `GenerationService._sanitize_input()` applies NFKC Unicode normalisation before regex matching to defeat homoglyph tricks. Defence-in-depth — not a security boundary. | `generation_service.py` |
| C-24 | **Chunk Overlap** | `IngestionPipeline._split_text()` uses 200-char paragraph-boundary overlap between chunks to reduce context loss at chunk boundaries. | `ingestion.py` |
| C-25 | **Docker Deployment** | Multi-stage `Dockerfile` (non-root user, healthcheck) + `docker-compose.yml` with persistent ChromaDB volume. | `Dockerfile`, `docker-compose.yml` |
| C-26 | **CI/CD Pipeline** | GitHub Actions: lint → unit tests (coverage gate) → eval regression guard on PRs to main. | `.github/workflows/ci.yml` |
